# -*- coding: utf-8 -*-
# English comments as per our rules

import logging
from openai import AsyncOpenAI 
from datetime import datetime, timezone 
import pytz 

logger = logging.getLogger(__name__)

async def execute_custom_prompt(
    api_key: str, 
    model_name: str, 
    user_text_prompt: str,
    max_tokens: int = 1000, 
    temperature: float = 0.7,
    system_message: str = "You are a helpful assistant. Respond in the language of the user's prompt if possible."
) -> str:
    # ... (This function remains the same as sakaibot_ai_processor_py_v3_focused_prompt)
    if not api_key or "YOUR_OPENROUTER_API_KEY_HERE" in api_key or len(api_key) < 50:
        logger.error("AI Processor: OpenRouter API key is not configured or seems invalid.")
        return "AI Error: OpenRouter API key not configured or invalid. Please check your config.ini."
    if not model_name:
        logger.error("AI Processor: OpenRouter model name is not configured.")
        return "AI Error: OpenRouter model name not configured. Please check your config.ini."
    if not user_text_prompt:
        logger.warning("AI Processor: Received an empty prompt.")
        return "AI Error: Prompt cannot be empty."
    try:
        client = AsyncOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
        messages_payload = []
        if system_message:
            messages_payload.append({"role": "system", "content": system_message})
        messages_payload.append({"role": "user", "content": user_text_prompt})
        logger.info(f"AI Processor: Sending prompt to AI model '{model_name}': '{user_text_prompt[:100]}...'")
        completion = await client.chat.completions.create(
            model=model_name, 
            messages=messages_payload,
            max_tokens=max_tokens,
            temperature=temperature,
            extra_headers={ 
                "HTTP-Referer": "http://localhost/sakaibot", 
                "X-Title": "SakaiBot" 
            }
        )
        response_text = completion.choices[0].message.content.strip()
        logger.info(f"AI Processor: Model '{model_name}' responded successfully for custom prompt.")
        return response_text
    except Exception as e:
        logger.error(f"AI Processor: Error calling OpenRouter API for custom prompt with model '{model_name}': {e}", exc_info=True)
        return f"AI Error: Could not get response from model '{model_name}'. Details: {str(e)}"


async def translate_text_with_phonetics(
    api_key: str,
    model_name: str,
    text_to_translate: str,
    target_language: str,
    source_language: str = "auto" 
) -> str:
    # ... (This function remains the same as sakaibot_ai_processor_py_v2_translate)
    if not text_to_translate:
        return "AI Error: No text provided for translation."
    if not target_language:
        return "AI Error: Target language not specified."
    phonetic_instruction = (
        f"After providing the translation into {target_language}, "
        f"also provide a simple phonetic pronunciation of the translated text using Persian alphabet characters, "
        f"enclosed in parentheses. For example, if the source text is 'mother' and the target language is German, "
        f"the output should be similar to: Mutter (ููุชุง). "
        f"If the source text is 'Wie geht es Ihnen?' and target language is English, "
        f"the output should be similar to: How are you? (ูุงู ุขุฑ ูุ)."
    )
    if source_language.lower() == "auto":
        prompt = (
            f"Detect the language of the following text and then translate it to '{target_language}'.\n"
            f"{phonetic_instruction}\n\n"
            f"Text to translate:\n\"{text_to_translate}\"\n\n"
            f"Output:"
        )
    else:
        prompt = (
            f"Translate the following text from '{source_language}' to '{target_language}'.\n"
            f"{phonetic_instruction}\n\n"
            f"Text to translate:\n\"{text_to_translate}\"\n\n"
            f"Output:"
        )
    logger.info(f"AI Processor: Requesting translation for '{text_to_translate[:50]}...' to {target_language} with phonetics.")
    system_msg_for_translation = "You are a multilingual translator. Provide the translation and then its Persian phonetic pronunciation in parentheses."
    translation_response = await execute_custom_prompt(
        api_key=api_key, model_name=model_name, user_text_prompt=prompt,
        max_tokens=len(text_to_translate) * 4 + 100, temperature=0.3,
        system_message=system_msg_for_translation 
    )
    if "AI Error:" not in translation_response:
        logger.info(f"AI Processor: Translation with phonetics successful for '{text_to_translate[:50]}...'.")
    else:
        logger.error(f"AI Processor: Translation with phonetics failed for '{text_to_translate[:50]}...'. Response: {translation_response}")
    return translation_response


async def analyze_conversation_messages(
    api_key: str, 
    model_name: str, 
    messages_data: list 
) -> str:
    """
    Analyzes a list of messages using the specified AI model via OpenRouter.
    Uses a more detailed Persian prompt for structured and specific analysis,
    including main topics and key points.
    """
    if not api_key or "YOUR_OPENROUTER_API_KEY_HERE" in api_key or len(api_key) < 50:
        logger.error("AI Processor: OpenRouter API key not configured for analysis.")
        return "AI Error: OpenRouter API key not configured for analysis."
    if not model_name:
        logger.error("AI Processor: OpenRouter model name not configured for analysis.")
        return "AI Error: OpenRouter model name not configured for analysis."
    if not messages_data:
        return "No messages provided for analysis."

    formatted_messages_for_prompt = []
    senders = set()
    timestamps = []

    for msg_info in messages_data:
        sender = msg_info.get('sender', 'Unknown')
        text = msg_info.get('text')
        timestamp_obj = msg_info.get('timestamp') 

        if text: 
            ts_aware = timestamp_obj
            if isinstance(timestamp_obj, datetime):
                if timestamp_obj.tzinfo is None:
                    ts_aware = pytz.utc.localize(timestamp_obj) 
                else:
                    ts_aware = timestamp_obj.astimezone(pytz.utc) 
            elif isinstance(timestamp_obj, (int, float)): 
                 ts_aware = datetime.fromtimestamp(timestamp_obj, tz=pytz.utc)
            
            formatted_messages_for_prompt.append(f"{sender}: {text}")
            senders.add(sender)
            if ts_aware: 
                timestamps.append(ts_aware)
    
    if not formatted_messages_for_prompt:
        return "No text messages found for analysis after formatting."

    combined_text_for_prompt = "\n".join(formatted_messages_for_prompt)
    num_messages = len(formatted_messages_for_prompt)
    num_senders = len(senders)
    duration_minutes = 0
    if len(timestamps) >= 2: 
        min_time = min(timestamps)
        max_time = max(timestamps)
        duration_minutes = int((max_time - min_time).total_seconds() / 60)
    elif len(timestamps) == 1: 
        duration_minutes = 0

    # --- New, more detailed Persian prompt for chat analysis ---
    prompt = (
        "ุดูุง ฺฉ ุฏุณุชุงุฑ ููุดููุฏ ุชุญููฺฏุฑ ูฺฉุงููุงุช ูุงุฑุณ ูุณุชุฏ. ูุทูุงู ูุชู ฺฏูุชฺฏู ุฒุฑ ุฑุง ุจู ุฏูุช ุจุฑุฑุณ ฺฉุฑุฏู ู ฺฉ ฺฏุฒุงุฑุด ุชุญูู ุฌุงูุน ู ุณุงุฎุชุงุฑุงูุชู ุจู ุฒุจุงู ูุงุฑุณ ุงุฑุงุฆู ุฏูุฏ. "
        "ููฺฏุงู ุชุญููุ ุจู ุฒููู ูุฑููฺฏุ ูุญู ูุญุงูุฑูโุงุ ู ุฑูุงุจุท ุงุญุชูุงู ุจู ฺฏููุฏฺฏุงู ุชูุฌู ูฺู ุฏุงุดุชู ุจุงุดุฏ. ุงุฒ ุชูุณุฑ ุชุญุชโุงูููุธ ุนุจุงุฑุงุช ฺฉู ููฺฉู ุงุณุช ุฏุฑ ุจุณุชุฑ ุฏูุณุชุงูู ุง ุดูุฎ ูุนูุง ูุชูุงูุช ุฏุงุดุชู ุจุงุดูุฏุ ูพุฑูุฒ ฺฉูุฏ.\n\n"
        "ฺฏุฒุงุฑุด ุดูุง ุจุงุฏ ุดุงูู ุจุฎุดโูุง ุฒุฑ ุจุง ููู ุนูุงูู ูุงุฑุณ ู ุจู ููู ุชุฑุชุจ ุจุงุดุฏ:\n\n"
        "1.  **ุฎูุงุตู ุงุฌุฑุง ูฺฉุงููู:**\n"
        "    ฺฉ ูพุงุฑุงฺฏุฑุงู ฺฉูุชุงู (ุญุฏุงฺฉุซุฑ ณ-ด ุฌููู) ฺฉู ฺฺฉุฏู ู ูุฏู ุงุตู ุงู ุจุฎุด ุงุฒ ฺฏูุชฺฏู ุฑุง ุจุงู ฺฉูุฏ. ุงุฒ ฺฉูโฺฏู ุจูพุฑูุฒุฏ ู ุจู ูููโุชุฑู ูุชุฌู ุง ูุฏู ฺฏูุชฺฏู ุงุดุงุฑู ฺฉูุฏ.\n\n"
        "2.  **ููุถูุนุงุช ุงุตู ู ูฺฉุงุช ฺฉูุฏ ูุทุฑุญ ุดุฏู:**\n"
        "    ููุถูุนุงุช ุงุตู ฺฉู ุฏุฑ ุงู ฺฏูุชฺฏู ููุฑุฏ ุจุญุซ ูุฑุงุฑ ฺฏุฑูุชูโุงูุฏ ุฑุง ุดูุงุณุง ู ุจู ุตูุฑุช ฺฉ ูุณุช ุดูุงุฑูโฺฏุฐุงุฑ ุดุฏู ุง ุจุง ุนููุงูโูุง ูุงุถุญ ุจุงู ฺฉูุฏ. ุจุฑุง ูุฑ ููุถูุน ุงุตูุ ูฺฉุงุช ฺฉูุฏุ ุงุทูุงุนุงุช ูููุ ุชุตููุงุช ฺฏุฑูุชู ุดุฏูุ ุง ุณูุงูุงุช ุงุตู ูุทุฑุญ ุดุฏู ุฑุง ุจู ุทูุฑ ุฎูุงุตู ู ุฏูู (ุจุง ุฌุฒุฆุงุช ูุฑุชุจุท ุงุฒ ูุชู) ุฐฺฉุฑ ููุงุฏ. ุณุน ฺฉูุฏ ุญุฏุงูู ฒ ุชุง ด ููุถูุน/ูฺฉุชู ฺฉูุฏ ุฑุง ุงุณุชุฎุฑุงุฌ ฺฉูุฏุ ูฺฏุฑ ุงูฺฉู ฺฏูุชฺฏู ุจุณุงุฑ ฺฉูุชุงู ุจุงุดุฏ.\n\n"
        "3.  **ุชุญูู ูุญู ู ุงุญุณุงุณุงุช ุบุงูุจ:**\n"
        "    ุงุจุชุฏุง ุฏุฑ ฺฉ ุฌูููุ ูุญู ฺฉู ู ุงุญุณุงุณุงุช ุบุงูุจ ุฏุฑ ุทูู ุงู ุจุฎุด ุงุฒ ูฺฉุงููู ุฑุง ุชูุตู ฺฉูุฏ (ูุซูุงู: ุฏูุณุชุงูู ู ูุดุชุงูุงููุ ุฑุณู ู ุฌุฏุ ุทูุฒุขูุฒ ุจุง ฺุงุดู ฺฉูุงูุ ูพุฑุชูุด ู ฺุงูุดุ ุฎูุซ ู ุงุทูุงุนโุฑุณุงู). ุงุฒ ุงููุฌโูุง ููุงุณุจ ุจุฑุง ููุงุด ุงุญุณุงุณุงุช ุงุณุชูุงุฏู ฺฉูุฏ (ูุซูุงู: ๐, ๐ข, ๐ก, โค๏ธ, ๐ค, ๐).\n"
        "    ุณูพุณุ ุจู ุทูุฑ ูุฎุชุตุฑ ุชูุถุญ ุฏูุฏ ฺฉู ฺฉุฏุงู ุจุฎุดโูุง ุงุฒ ฺฏูุชฺฏู ุง ฺฉุฏุงู ุนุจุงุฑุงุช ุดูุง ุฑุง ุจู ุงู ุชุดุฎุต ุฑุณุงูุฏูโุงูุฏ. ุงฺฏุฑ ุชุบุฑุงุช ูุงุจู ุชูุฌู ุฏุฑ ูุญู ุง ุงุญุณุงุณุงุช ุฏุฑ ุทูู ฺฏูุชฺฏู ูุฌูุฏ ุฏุงุดุชูุ ุจู ุขู ุงุดุงุฑู ฺฉูุฏ.\n\n"
        "4.  **ุงูุฏุงูุงุชุ ุชุตููุงุชุ ู ูุฑุงุฑูุง (ุฑูุฏุงุฏูุง):**\n"
        "    ูุฑฺฏููู ุงูุฏุงู ุงูุฌุงู ุดุฏูุ ุชุตูู ฺฏุฑูุชู ุดุฏูุ ูุฑุงุฑ ููุงูุงุช ุชูุธู ุดุฏูุ ุง ูุธูู ูุดุฎุต ุดุฏู ุฏุฑ ุงู ฺฏูุชฺฏู ุฑุง ุจู ุตูุฑุช ฺฉ ูุณุช ููุฑุฏ (bullet points) ุจุงู ฺฉูุฏ. ุงฺฏุฑ ุฒูุงูุ ูฺฉุงูุ ุง ูุณุฆูู ุฎุงุต ุจุฑุง ูุฑ ููุฑุฏ ุฐฺฉุฑ ุดุฏูุ ุขู ุฑุง ูุฒ ุงุถุงูู ฺฉูุฏ. ููุงุฑุฏ ุฑุง ุจุฑ ุงุณุงุณ ูุทุนุช (ุงุจุชุฏุง ููุงุฑุฏ ูุทุนุ ุณูพุณ ูพุดููุงุฏุงุช) ูุฑุชุจ ฺฉูุฏ.\n\n"
        "ุขูุงุฑ ูฺฉุงููู: ุงู ฺฏูุชฺฏู ุดุงูู {num_messages} ูพุงู ุจู {num_senders} ููุฑ ุฏุฑ ุท ุญุฏูุฏ {duration_minutes} ุฏููู ุจูุฏู ุงุณุช.\n\n"
        "ูพุงูโูุง ุฌูุช ุชุญูู:\n"
        "```\n"
        f"{combined_text_for_prompt}\n"
        "```\n\n"
        "ุชุญูู ูุงุฑุณ:"
    ).format(num_messages=num_messages, num_senders=num_senders, duration_minutes=duration_minutes)

    logger.info(f"AI Processor: Sending conversation ({num_messages} messages) for DETAILED analysis to model '{model_name}'.")
    
    system_msg_for_analysis = "You are a professional Persian chat analyst. Provide a comprehensive and structured report based on the user's detailed instructions, ensuring all requested sections are covered accurately and in Persian."

    analysis_response = await execute_custom_prompt(
        api_key=api_key,
        model_name=model_name,
        user_text_prompt=prompt,
        max_tokens=2000, # Increased max_tokens for more detailed analysis
        temperature=0.4, # Slightly lower temperature for more factual and structured output
        system_message=system_msg_for_analysis
    )

    if "AI Error:" not in analysis_response:
        logger.info(f"AI Processor: Detailed analysis successful for {num_messages} messages.")
    else:
        logger.error(f"AI Processor: Detailed analysis failed. Response: {analysis_response}")
        
    return analysis_response


# Standalone Test Block
if __name__ == '__main__':
    import asyncio
    import pytz 
    from datetime import timedelta

    STANDALONE_TEST_API_KEY = "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" 
    STANDALONE_TEST_MODEL = "deepseek/deepseek-chat" 

    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logging.getLogger("ai_processor").setLevel(logging.DEBUG)


    async def run_standalone_tests():
        if "YOUR_OPENROUTER_API_KEY_HERE" in STANDALONE_TEST_API_KEY or not STANDALONE_TEST_API_KEY or len(STANDALONE_TEST_API_KEY) < 50:
            print("Please set your actual OpenRouter API key in STANDALONE_TEST_API_KEY to run the test.")
            return

        print(f"\n--- Running Standalone AI Processor Tests (Model: {STANDALONE_TEST_MODEL}) ---")
        
        # Test /analyze with the new prompt
        print("\nTesting /analyze (with new detailed prompt):")
        sample_messages_data_for_analyze = [
            {'sender': "ุณูุง", 'text': "ุณูุงู ูููู ุฌุงูุ ุฎูุจุ ุจุฑุง ูพุฑูฺู SakaiBot ฺู ุฎุจุฑุ ุชููุณุช ุจุฎุด AI ุฑู ุดุฑูุน ฺฉูุ", 'timestamp': datetime.now(pytz.utc) - timedelta(minutes=10)},
            {'sender': "ูููู", 'text': "ุณูุงู ุณูุงุ ุขุฑู ููููู. ฺฉู ุฏุฑฺฏุฑ ุจูุฏู ูู ุจุฎุด `/prompt` ุฑู ุชูุฑุจุงู ุขูุงุฏู ฺฉุฑุฏู. ููุท ูููุฏู ุชุณุช ููุง ู ุงุฏุบุงูุด.", 'timestamp': datetime.now(pytz.utc) - timedelta(minutes=8)},
            {'sender': "ุณูุง", 'text': "ุนุงูู! ุฎู ุฎูุจู. ุจุฑุง `/analyze` ูู ุจุฑูุงูู ุง ุฏุงุฑุ ุจู ูุธุฑู ุฎู ฺฉุงุฑุจุฑุฏ ูุดู ุงฺฏู ุจุชููู ูฺฉุงุช ููู ุฑู ุฎูุจ ุฏุฑุจุงุฑู.", 'timestamp': datetime.now(pytz.utc) - timedelta(minutes=5)},
            {'sender': "ูููู", 'text': "ุฏููุง! ุงุชูุงูุง ุฏุงุดุชู ุจู ูพุฑุงููพุชุด ูฺฉุฑ ูโฺฉุฑุฏู. ุจุงุฏ ุฎู ุฏูู ุจุงุดู ฺฉู ููุท ฺฉูโฺฏู ูฺฉูู. ุชุตูู ฺฏุฑูุชู ุงุฒุด ุจุฎูุงู ููุถูุนุงุช ุงุตู ู ูฺฉุงุช ฺฉูุฏ ุฑู ูุณุช ฺฉูู.", 'timestamp': datetime.now(pytz.utc) - timedelta(minutes=3)},
            {'sender': "ุณูุง", 'text': "ูููโุงูุนุงุฏูโุณุช! ูพุณ ูุฑุงุฑ ุดุฏ ฺฉู ุจุฑุง ุชุญููุ ููุถูุนุงุช ุงุตู ู ูฺฉุงุช ฺฉูุฏ ุฑู ูู ุงุถุงูู ฺฉูู. ุชุญูู ุงุญุณุงุณุงุช ู ุฑูุฏุงุฏูุง ูู ฺฉู ุณุฑ ุฌุงุดู. ุฏุฑุณุชูุ", 'timestamp': datetime.now(pytz.utc)},
            {'sender': "ูููู", 'text': "ุขุฑู ุฏููุงู. ุฌูุนู ุณุงุนุช ต ุนุตุฑ ููุช ุฏุงุฑ ู ุฌูุณู ุขููุงู ุฏุงุดุชู ุจุงุดู ุจุฑุง ุจุฑุฑุณ ููุง ู ุชุณุชุ", 'timestamp': datetime.now(pytz.utc) + timedelta(minutes=2)},
            {'sender': "ุณูุง", 'text': "ุฌูุนู ต ุงูฺฉู. ูู ูุณุชู. ๐", 'timestamp': datetime.now(pytz.utc) + timedelta(minutes=4)}
        ]
        response_analyze = await analyze_conversation_messages(
            STANDALONE_TEST_API_KEY, 
            STANDALONE_TEST_MODEL, 
            sample_messages_data_for_analyze
        )
        print(f"AI Response for /analyze:\n{response_analyze}")

    asyncio.run(run_standalone_tests())
