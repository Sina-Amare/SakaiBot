# -*- coding: utf-8 -*-
# English comments as per our rules

import logging
from openai import AsyncOpenAI 
from datetime import datetime, timezone 
import pytz 

logger = logging.getLogger(__name__)

async def execute_custom_prompt(
    api_key: str, 
    model_name: str, 
    user_text_prompt: str,
    max_tokens: int = 1500, 
    temperature: float = 0.7,
    system_message: str or None = None 
) -> str:
    # ... (This function remains the same as sakaibot_ai_processor_py_v7_direct_prompt)
    if not api_key or "YOUR_OPENROUTER_API_KEY_HERE" in api_key or len(api_key) < 50:
        logger.error("AI Processor: OpenRouter API key is not configured or seems invalid.")
        return "AI Error: OpenRouter API key not configured or invalid. Please check your config.ini."
    if not model_name:
        logger.error("AI Processor: OpenRouter model name is not configured.")
        return "AI Error: OpenRouter model name not configured. Please check your config.ini."
    if not user_text_prompt:
        logger.warning("AI Processor: Received an empty prompt.")
        return "AI Error: Prompt cannot be empty."
    try:
        client = AsyncOpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=api_key,
        )
        messages_payload = []
        if system_message and system_message.strip():
            logger.debug(f"AI Processor: Using System Message: '{system_message}'")
            messages_payload.append({"role": "system", "content": system_message})
        else:
            logger.debug("AI Processor: No specific system message provided for this prompt call.")
        messages_payload.append({"role": "user", "content": user_text_prompt})
        logger.info(f"AI Processor: Sending prompt to AI model '{model_name}'. System message used: {'Yes' if system_message and system_message.strip() else 'No'}. Prompt starts with: '{user_text_prompt[:100]}...' (max_tokens: {max_tokens}, temp: {temperature})")
        completion = await client.chat.completions.create(
            model=model_name, 
            messages=messages_payload,
            max_tokens=max_tokens,
            temperature=temperature,
            extra_headers={ 
                "HTTP-Referer": "http://localhost/sakaibot", 
                "X-Title": "SakaiBot" 
            }
        )
        response_text = completion.choices[0].message.content.strip()
        logger.info(f"AI Processor: Model '{model_name}' responded successfully for custom prompt.")
        return response_text
    except Exception as e:
        logger.error(f"AI Processor: Error calling OpenRouter API for custom prompt with model '{model_name}': {e}", exc_info=True)
        return f"AI Error: Could not get response from model '{model_name}'. Details: {str(e)}"


async def translate_text_with_phonetics(
    api_key: str,
    model_name: str,
    text_to_translate: str,
    target_language: str,
    source_language: str = "auto" 
) -> str:
    # ... (This function remains the same as sakaibot_ai_processor_py_v5_analyze_format_fix)
    if not text_to_translate:
        return "AI Error: No text provided for translation."
    if not target_language:
        return "AI Error: Target language not specified."
    phonetic_instruction = (
        f"After providing the translation into {target_language}, "
        f"also provide a simple phonetic pronunciation of the translated text using Persian alphabet characters, "
        f"enclosed in parentheses. For example, if the source text is 'mother' and the target language is German, "
        f"the output should be similar to: Mutter (Ù…ÙˆØªØ§). "
        f"If the source text is 'Wie geht es Ihnen?' and target language is English, "
        f"the output should be similar to: How are you? (Ù‡Ø§Ùˆ Ø¢Ø± ÛŒÙˆØŸ)."
    )
    if source_language.lower() == "auto":
        prompt = (
            f"Detect the language of the following text and then translate it to '{target_language}'.\n"
            f"{phonetic_instruction}\n\n"
            f"Text to translate:\n\"{text_to_translate}\"\n\n"
            f"Output:"
        )
    else:
        prompt = (
            f"Translate the following text from '{source_language}' to '{target_language}'.\n"
            f"{phonetic_instruction}\n\n"
            f"Text to translate:\n\"{text_to_translate}\"\n\n"
            f"Output:"
        )
    logger.info(f"AI Processor: Requesting translation for '{text_to_translate[:50]}...' to {target_language} with phonetics.")
    system_msg_for_translation = "You are a multilingual translator. Provide the translation and then its Persian phonetic pronunciation in parentheses."
    translation_response = await execute_custom_prompt(
        api_key=api_key, model_name=model_name, user_text_prompt=prompt,
        max_tokens=len(text_to_translate) * 4 + 150, temperature=0.2,
        system_message=system_msg_for_translation 
    )
    if "AI Error:" not in translation_response:
        logger.info(f"AI Processor: Translation with phonetics successful for '{text_to_translate[:50]}...'.")
    else:
        logger.error(f"AI Processor: Translation with phonetics failed for '{text_to_translate[:50]}...'. Response: {translation_response}")
    return translation_response


async def analyze_conversation_messages(
    api_key: str, 
    model_name: str, 
    messages_data: list 
) -> str:
    # ... (This function remains the same as sakaibot_ai_processor_py_v5_analyze_format_fix)
    if not api_key or "YOUR_OPENROUTER_API_KEY_HERE" in api_key or len(api_key) < 50:
        logger.error("AI Processor: OpenRouter API key not configured for analysis.")
        return "AI Error: OpenRouter API key not configured for analysis."
    if not model_name:
        logger.error("AI Processor: OpenRouter model name not configured for analysis.")
        return "AI Error: OpenRouter model name not configured for analysis."
    if not messages_data:
        return "No messages provided for analysis."
    formatted_messages_for_prompt = []
    senders = set()
    timestamps = []
    for msg_info in messages_data:
        sender = msg_info.get('sender', 'Unknown')
        text = msg_info.get('text')
        timestamp_obj = msg_info.get('timestamp') 
        if text: 
            ts_aware = timestamp_obj
            if isinstance(timestamp_obj, datetime):
                if timestamp_obj.tzinfo is None: ts_aware = pytz.utc.localize(timestamp_obj) 
                else: ts_aware = timestamp_obj.astimezone(pytz.utc) 
            elif isinstance(timestamp_obj, (int, float)): ts_aware = datetime.fromtimestamp(timestamp_obj, tz=pytz.utc)
            formatted_messages_for_prompt.append(f"{sender}: {text}")
            senders.add(sender)
            if ts_aware: timestamps.append(ts_aware)
    if not formatted_messages_for_prompt: return "No text messages found for analysis after formatting."
    combined_text_for_prompt_var = "\n".join(formatted_messages_for_prompt)
    num_messages = len(formatted_messages_for_prompt); num_senders = len(senders); duration_minutes = 0
    if len(timestamps) >= 2: 
        min_time = min(timestamps); max_time = max(timestamps)
        duration_minutes = int((max_time - min_time).total_seconds() / 60)
    elif len(timestamps) == 1: duration_minutes = 0
    prompt_template = (
        "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ù…Ú©Ø§Ù„Ù…Ø§Øª ÙØ§Ø±Ø³ÛŒ Ù‡Ø³ØªÛŒØ¯. Ù„Ø·ÙØ§Ù‹ Ù…ØªÙ† Ú¯ÙØªÚ¯ÙˆÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡ Ø¯Ù‚Øª Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø±Ø¯Ù‡ Ùˆ ÛŒÚ© Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ÛŒ Ø¬Ø§Ù…Ø¹ Ùˆ Ø³Ø§Ø®ØªØ§Ø±ÛŒØ§ÙØªÙ‡ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯. "
        "Ù‡Ù†Ú¯Ø§Ù… ØªØ­Ù„ÛŒÙ„ØŒ Ø¨Ù‡ Ø²Ù…ÛŒÙ†Ù‡ ÙØ±Ù‡Ù†Ú¯ÛŒØŒ Ù„Ø­Ù† Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒØŒ Ùˆ Ø±ÙˆØ§Ø¨Ø· Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø¨ÛŒÙ† Ú¯ÙˆÛŒÙ†Ø¯Ú¯Ø§Ù† ØªÙˆØ¬Ù‡ ÙˆÛŒÚ˜Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒØ¯. Ø§Ø² ØªÙØ³ÛŒØ± ØªØ­Øªâ€ŒØ§Ù„Ù„ÙØ¸ÛŒ Ø¹Ø¨Ø§Ø±Ø§ØªÛŒ Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± Ø¨Ø³ØªØ± Ø¯ÙˆØ³ØªØ§Ù†Ù‡ ÛŒØ§ Ø´ÙˆØ®ÛŒ Ù…Ø¹Ù†Ø§ÛŒ Ù…ØªÙØ§ÙˆØªÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯ØŒ Ù¾Ø±Ù‡ÛŒØ² Ú©Ù†ÛŒØ¯.\n\n"
        "Ú¯Ø²Ø§Ø±Ø´ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø¨Ø§ Ù‡Ù…ÛŒÙ† Ø¹Ù†Ø§ÙˆÛŒÙ† ÙØ§Ø±Ø³ÛŒ Ùˆ Ø¨Ù‡ Ù‡Ù…ÛŒÙ† ØªØ±ØªÛŒØ¨ Ø¨Ø§Ø´Ø¯:\n\n"
        "1.  **Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§ÛŒÛŒ Ù…Ú©Ø§Ù„Ù…Ù‡:**\n"
        "    ÛŒÚ© Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ú©ÙˆØªØ§Ù‡ (Ø­Ø¯Ø§Ú©Ø«Ø± Û³-Û´ Ø¬Ù…Ù„Ù‡) Ú©Ù‡ Ú†Ú©ÛŒØ¯Ù‡ Ùˆ Ù‡Ø¯Ù Ø§ØµÙ„ÛŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø² Ú¯ÙØªÚ¯Ùˆ Ø±Ø§ Ø¨ÛŒØ§Ù† Ú©Ù†Ø¯. Ø§Ø² Ú©Ù„ÛŒâ€ŒÚ¯ÙˆÛŒÛŒ Ø¨Ù¾Ø±Ù‡ÛŒØ²ÛŒØ¯ Ùˆ Ø¨Ù‡ Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡ ÛŒØ§ Ù‡Ø¯Ù Ú¯ÙØªÚ¯Ùˆ Ø§Ø´Ø§Ø±Ù‡ Ú©Ù†ÛŒØ¯.\n\n"
        "2.  **Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø§ØµÙ„ÛŒ Ùˆ Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø·Ø±Ø­ Ø´Ø¯Ù‡:**\n"
        "    Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø§ØµÙ„ÛŒ Ú©Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ú¯ÙØªÚ¯Ùˆ Ù…ÙˆØ±Ø¯ Ø¨Ø­Ø« Ù‚Ø±Ø§Ø± Ú¯Ø±ÙØªÙ‡â€ŒØ§Ù†Ø¯ Ø±Ø§ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© Ù„ÛŒØ³Øª Ø´Ù…Ø§Ø±Ù‡â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ ÛŒØ§ Ø¨Ø§ Ø¹Ù†ÙˆØ§Ù†â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ø¶Ø­ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯. Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…ÙˆØ¶ÙˆØ¹ Ø§ØµÙ„ÛŒØŒ Ù†Ú©Ø§Øª Ú©Ù„ÛŒØ¯ÛŒØŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ù‡Ù…ØŒ ØªØµÙ…ÛŒÙ…Ø§Øª Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ØŒ ÛŒØ§ Ø³ÙˆØ§Ù„Ø§Øª Ø§ØµÙ„ÛŒ Ù…Ø·Ø±Ø­ Ø´Ø¯Ù‡ Ø±Ø§ Ø¨Ù‡ Ø·ÙˆØ± Ø®Ù„Ø§ØµÙ‡ Ùˆ Ø¯Ù‚ÛŒÙ‚ (Ø¨Ø§ Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø±ØªØ¨Ø· Ø§Ø² Ù…ØªÙ†) Ø°Ú©Ø± Ù†Ù…Ø§ÛŒÛŒØ¯. Ø³Ø¹ÛŒ Ú©Ù†ÛŒØ¯ Ø­Ø¯Ø§Ù‚Ù„ Û² ØªØ§ Û´ Ù…ÙˆØ¶ÙˆØ¹/Ù†Ú©ØªÙ‡ Ú©Ù„ÛŒØ¯ÛŒ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒØ¯ØŒ Ù…Ú¯Ø± Ø§ÛŒÙ†Ú©Ù‡ Ú¯ÙØªÚ¯Ùˆ Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆØªØ§Ù‡ Ø¨Ø§Ø´Ø¯.\n\n"
        "3.  **ØªØ­Ù„ÛŒÙ„ Ù„Ø­Ù† Ùˆ Ø§Ø­Ø³Ø§Ø³Ø§Øª ØºØ§Ù„Ø¨:**\n"
        "    Ø§Ø¨ØªØ¯Ø§ Ø¯Ø± ÛŒÚ© Ø¬Ù…Ù„Ù‡ØŒ Ù„Ø­Ù† Ú©Ù„ÛŒ Ùˆ Ø§Ø­Ø³Ø§Ø³Ø§Øª ØºØ§Ù„Ø¨ Ø¯Ø± Ø·ÙˆÙ„ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø§Ø² Ù…Ú©Ø§Ù„Ù…Ù‡ Ø±Ø§ ØªÙˆØµÛŒÙ Ú©Ù†ÛŒØ¯ (Ù…Ø«Ù„Ø§Ù‹: Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ù…Ø´ØªØ§Ù‚Ø§Ù†Ù‡ØŒ Ø±Ø³Ù…ÛŒ Ùˆ Ø¬Ø¯ÛŒØŒ Ø·Ù†Ø²Ø¢Ù…ÛŒØ² Ø¨Ø§ Ú†Ø§Ø´Ù†ÛŒ Ú©Ù†Ø§ÛŒÙ‡ØŒ Ù¾Ø±ØªÙ†Ø´ Ùˆ Ú†Ø§Ù„Ø´ÛŒØŒ Ø®Ù†Ø«ÛŒ Ùˆ Ø§Ø·Ù„Ø§Ø¹â€ŒØ±Ø³Ø§Ù†ÛŒ). Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ù…Ø«Ù„Ø§Ù‹: ðŸ˜Š, ðŸ˜¢, ðŸ˜¡, â¤ï¸, ðŸ¤”, ðŸ˜).\n"
        "    Ø³Ù¾Ø³ØŒ Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø®ØªØµØ± ØªÙˆØ¶ÛŒØ­ Ø¯Ù‡ÛŒØ¯ Ú©Ù‡ Ú©Ø¯Ø§Ù… Ø¨Ø®Ø´â€ŒÙ‡Ø§ Ø§Ø² Ú¯ÙØªÚ¯Ùˆ ÛŒØ§ Ú©Ø¯Ø§Ù… Ø¹Ø¨Ø§Ø±Ø§Øª Ø´Ù…Ø§ Ø±Ø§ Ø¨Ù‡ Ø§ÛŒÙ† ØªØ´Ø®ÛŒØµ Ø±Ø³Ø§Ù†Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ø§Ú¯Ø± ØªØºÛŒÛŒØ±Ø§Øª Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¯Ø± Ù„Ø­Ù† ÛŒØ§ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ø¯Ø± Ø·ÙˆÙ„ Ú¯ÙØªÚ¯Ùˆ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ØŒ Ø¨Ù‡ Ø¢Ù† Ø§Ø´Ø§Ø±Ù‡ Ú©Ù†ÛŒØ¯.\n\n"
        "4.  **Ø§Ù‚Ø¯Ø§Ù…Ø§ØªØŒ ØªØµÙ…ÛŒÙ…Ø§ØªØŒ Ùˆ Ù‚Ø±Ø§Ø±Ù‡Ø§ (Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§):**\n"
        "    Ù‡Ø±Ú¯ÙˆÙ†Ù‡ Ø§Ù‚Ø¯Ø§Ù… Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ØŒ ØªØµÙ…ÛŒÙ… Ú¯Ø±ÙØªÙ‡ Ø´Ø¯Ù‡ØŒ Ù‚Ø±Ø§Ø± Ù…Ù„Ø§Ù‚Ø§Øª ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯Ù‡ØŒ ÛŒØ§ ÙˆØ¸ÛŒÙÙ‡ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ú¯ÙØªÚ¯Ùˆ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ÛŒÚ© Ù„ÛŒØ³Øª Ù…ÙˆØ±Ø¯ÛŒ (bullet points) Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯. Ø§Ú¯Ø± Ø²Ù…Ø§Ù†ØŒ Ù…Ú©Ø§Ù†ØŒ ÛŒØ§ Ù…Ø³Ø¦ÙˆÙ„ Ø®Ø§ØµÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù…ÙˆØ±Ø¯ Ø°Ú©Ø± Ø´Ø¯Ù‡ØŒ Ø¢Ù† Ø±Ø§ Ù†ÛŒØ² Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯. Ù…ÙˆØ§Ø±Ø¯ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚Ø·Ø¹ÛŒØª (Ø§Ø¨ØªØ¯Ø§ Ù…ÙˆØ§Ø±Ø¯ Ù‚Ø·Ø¹ÛŒØŒ Ø³Ù¾Ø³ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª) Ù…Ø±ØªØ¨ Ú©Ù†ÛŒØ¯.\n\n"
        "Ø¢Ù…Ø§Ø± Ù…Ú©Ø§Ù„Ù…Ù‡: Ø§ÛŒÙ† Ú¯ÙØªÚ¯Ùˆ Ø´Ø§Ù…Ù„ {num_messages} Ù¾ÛŒØ§Ù… Ø¨ÛŒÙ† {num_senders} Ù†ÙØ± Ø¯Ø± Ø·ÛŒ Ø­Ø¯ÙˆØ¯ {duration_minutes} Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª.\n\n"
        "Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø¬Ù‡Øª ØªØ­Ù„ÛŒÙ„:\n"
        "```\n"
        "{actual_chat_messages}\n" 
        "```\n\n"
        "ØªØ­Ù„ÛŒÙ„ ÙØ§Ø±Ø³ÛŒ:"
    )
    prompt = prompt_template.format(num_messages=num_messages, num_senders=num_senders, duration_minutes=duration_minutes, actual_chat_messages=combined_text_for_prompt_var)
    logger.info(f"AI Processor: Sending conversation ({num_messages} messages) for DETAILED analysis to model '{model_name}'.")
    system_msg_for_analysis = "You are a professional Persian chat analyst. Provide a comprehensive and structured report based on the user's detailed instructions, ensuring all requested sections are covered accurately and in Persian."
    analysis_response = await execute_custom_prompt(api_key=api_key, model_name=model_name, user_text_prompt=prompt, max_tokens=2000, temperature=0.4, system_message=system_msg_for_analysis)
    if "AI Error:" not in analysis_response: logger.info(f"AI Processor: Detailed analysis successful for {num_messages} messages.")
    else: logger.error(f"AI Processor: Detailed analysis failed. Response: {analysis_response}")
    return analysis_response

async def answer_question_from_chat_history(
    api_key: str,
    model_name: str,
    messages_data: list, # Expected: list of dicts {'sender': str, 'text': str, 'timestamp': datetime}
    user_question: str
) -> str:
    """
    Answers a specific user question based on the provided chat history.
    Uses a professional and colloquial tone.
    """
    if not api_key or "YOUR_OPENROUTER_API_KEY_HERE" in api_key or len(api_key) < 50:
        logger.error("AI Processor: OpenRouter API key not configured for question answering.")
        return "AI Error: OpenRouter API key not configured."
    if not model_name:
        logger.error("AI Processor: OpenRouter model name not configured for question answering.")
        return "AI Error: OpenRouter model name not configured."
    if not messages_data:
        return "No chat history provided to answer the question."
    if not user_question:
        return "No question provided to answer."

    formatted_messages_for_prompt = []
    for msg_info in messages_data:
        sender = msg_info.get('sender', 'Unknown')
        text = msg_info.get('text')
        if text:
            formatted_messages_for_prompt.append(f"{sender}: {text}")
    
    if not formatted_messages_for_prompt:
        return "No text messages found in the provided history."

    combined_history_text = "\n".join(formatted_messages_for_prompt)

    # Construct the prompt for the AI
    prompt = (
        "Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ Ø¨Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ§Ø±ÛŒØ®Ú†Ù‡ ÛŒÚ© Ú¯ÙØªÚ¯Ùˆ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯.\n"
        "Ù„Ø·ÙØ§Ù‹ Ø¨Ø§ Ù„Ø­Ù†ÛŒ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ø§Ù…Ø§ Ø®ÙˆØ¯Ù…ÙˆÙ†ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ØŒ Ùˆ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯ÙˆÛŒ Ø²ÛŒØ±ØŒ Ø¨Ù‡ Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø± Ù¾Ø§Ø³Ø® Ø¯Ù‡ÛŒØ¯.\n"
        "Ø§Ú¯Ø± Ù¾Ø§Ø³Ø® Ø³ÙˆØ§Ù„ Ø¯Ø± ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³ØªØŒ Ø¨Ù‡ ÙˆØ¶ÙˆØ­ Ø¨ÛŒØ§Ù† Ú©Ù†ÛŒØ¯ Ú©Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ø¯Ø± Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø´Ø¯Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.\n\n"
        "ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ú¯ÙØªÚ¯Ùˆ (Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø§ÙˆÙ„ Ø¢Ù…Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ù…Ø¹Ú©ÙˆØ³ Ø²Ù…Ø§Ù†ÛŒ):\n"
        "```\n"
        f"{combined_history_text}\n"
        "```\n\n"
        f"Ø³ÙˆØ§Ù„ Ú©Ø§Ø±Ø¨Ø±: {user_question}\n\n"
        "Ù¾Ø§Ø³Ø® Ø´Ù…Ø§ (Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ):"
    )

    logger.info(f"AI Processor: Answering question '{user_question[:50]}...' based on {len(formatted_messages_for_prompt)} messages using model '{model_name}'.")

    # System message to guide the AI's role and tone
    system_msg_for_qa = "You are an AI assistant specialized in answering questions based on provided chat history. Maintain a professional yet friendly and colloquial tone. If the answer isn't in the history, state that clearly. Respond in Persian unless the question implies another language."

    answer = await execute_custom_prompt(
        api_key=api_key,
        model_name=model_name,
        user_text_prompt=prompt,
        max_tokens=1000, # Adjust as needed, can be shorter for direct answers
        temperature=0.5, # A balance between factual and natural
        system_message=system_msg_for_qa
    )

    if "AI Error:" not in answer:
        logger.info(f"AI Processor: Successfully answered question '{user_question[:50]}...'.")
    else:
        logger.error(f"AI Processor: Failed to answer question '{user_question[:50]}...'. Response: {answer}")
        
    return answer


# Standalone Test Block
if __name__ == '__main__':
    # ... (Standalone test block can be updated to include answer_question_from_chat_history) ...
    import asyncio
    import pytz 
    from datetime import timedelta

    STANDALONE_TEST_API_KEY = "YOUR_OPENROUTER_API_KEY_FOR_STANDALONE_TESTING_ONLY" 
    STANDALONE_TEST_MODEL = "deepseek/deepseek-chat" 

    logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    logging.getLogger("ai_processor").setLevel(logging.DEBUG)


    async def run_standalone_tests():
        if "YOUR_OPENROUTER_API_KEY_FOR_STANDALONE_TESTING_ONLY" in STANDALONE_TEST_API_KEY or \
           not STANDALONE_TEST_API_KEY or len(STANDALONE_TEST_API_KEY) < 50:
            print("Please set your actual OpenRouter API key in STANDALONE_TEST_API_KEY to run the test.")
            return

        print(f"\n--- Running Standalone AI Processor Tests (Model: {STANDALONE_TEST_MODEL}) ---")
        
        # Test /tellme
        print("\nTesting /tellme (answer_question_from_chat_history):")
        sample_history_for_tellme = [
            {'sender': "Ø³ÛŒÙ†Ø§", 'text': "ÛŒØ§Ø¯ØªÙ‡ Ù‡ÙØªÙ‡ Ù¾ÛŒØ´ Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ø§ÙˆÙ† Ø±Ø³ØªÙˆØ±Ø§Ù† Ø§ÛŒØªØ§Ù„ÛŒØ§ÛŒÛŒ Ø¬Ø¯ÛŒØ¯Ù‡ Ø­Ø±Ù Ù…ÛŒâ€ŒØ²Ø¯ÛŒÙ…ØŸ Ø§Ø³Ù…Ø´ Ú†ÛŒ Ø¨ÙˆØ¯ØŸ", 'timestamp': datetime.now(pytz.utc) - timedelta(days=7, hours=1)},
            {'sender': "Ù‡ÙˆÙ…Ù†", 'text': "Ø¢Ù‡Ø§ØŒ Ø¢Ø±Ù‡! ÙÚ©Ø± Ú©Ù†Ù… 'Ù„Ø§Ù¾ÛŒØªØ²Ø§' Ø¨ÙˆØ¯. Ú¯ÙØªÛŒÙ… Ø¢Ø®Ø± Ø§ÛŒÙ† Ù‡ÙØªÙ‡ Ø¨Ø±ÛŒÙ… Ø§Ù…ØªØ­Ø§Ù†Ø´ Ú©Ù†ÛŒÙ….", 'timestamp': datetime.now(pytz.utc) - timedelta(days=7)},
            {'sender': "ØºØ²Ù„", 'text': "Ù…Ù†Ù… Ø®ÛŒÙ„ÛŒ ØªØ¹Ø±ÛŒÙØ´Ùˆ Ø´Ù†ÛŒØ¯Ù…! Ù¾Ø§Ø³ØªØ§Ù‡Ø§Ø´ Ù…ÛŒÚ¯Ù† Ø¹Ø§Ù„ÛŒÙ‡.", 'timestamp': datetime.now(pytz.utc) - timedelta(days=6)},
            {'sender': "Ø³ÛŒÙ†Ø§", 'text': "Ù¾Ø³ Ù‚Ø±Ø§Ø±Ù…ÙˆÙ† Ø´Ø¯ Ø¬Ù…Ø¹Ù‡ Ø´Ø¨ØŒ Ø±Ø³ØªÙˆØ±Ø§Ù† Ù„Ø§Ù¾ÛŒØªØ²Ø§. Ù…Ù† Ø±Ø²Ø±Ùˆ Ù…ÛŒâ€ŒÚ©Ù†Ù….", 'timestamp': datetime.now(pytz.utc) - timedelta(days=5)},
            {'sender': "Ù‡ÙˆÙ…Ù†", 'text': "Ø¹Ø§Ù„ÛŒÙ‡ØŒ ÙÙ‚Ø· Ù…Ù† Ø´Ø§ÛŒØ¯ ÛŒÙ‡ Ú©Ù… Ø¯ÛŒØ±ØªØ± Ø¨Ø±Ø³Ù…ØŒ Ø­Ø¯ÙˆØ¯ Û¹.", 'timestamp': datetime.now(pytz.utc) - timedelta(days=5, hours=-1)},
            {'sender': "ØºØ²Ù„", 'text': "Ù…Ø´Ú©Ù„ÛŒ Ù†ÛŒØ³Øª Ù‡ÙˆÙ…Ù† Ø¬Ø§Ù†. Ø³ÛŒÙ†Ø§ØŒ ØªÙˆ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ø§ÙˆÙ† ÙÛŒÙ„Ù… ØªØ±Ø³Ù†Ø§Ú©Ù‡ Ú©Ù‡ Ú¯ÙØªÛŒ Ú†ÛŒØ²ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯ÛŒØŸ", 'timestamp': datetime.now(pytz.utc) - timedelta(days=4)},
            {'sender': "Ø³ÛŒÙ†Ø§", 'text': "Ø¢Ø±Ù‡ØŒ Ø§Ø³Ù…Ø´ 'Ø³Ú©ÙˆØª Ø¨Ø±Ù‡â€ŒÙ‡Ø§' Ø¨ÙˆØ¯ØŒ ÙˆÙ„ÛŒ Ø®ÛŒÙ„ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒÙ‡. ÛŒÙ‡ ÙÛŒÙ„Ù… Ø¬Ø¯ÛŒØ¯ØªØ± Ø¨Ù‡ Ø§Ø³Ù… 'Ù†Ø¬ÙˆØ§Ú¯Ø±' Ù‡Ù… Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù… Ú©Ù‡ Ø§Ù…ØªÛŒØ§Ø²Ø§Ø´ Ø®ÙˆØ¨Ù‡.", 'timestamp': datetime.now(pytz.utc) - timedelta(days=3)}
        ]
        
        question1 = "Ø§Ø³Ù… Ø±Ø³ØªÙˆØ±Ø§Ù†ÛŒ Ú©Ù‡ Ù‚Ø±Ø§Ø± Ø´Ø¯ Ø¨Ø±ÛŒÙ… Ú†ÛŒ Ø¨ÙˆØ¯ Ùˆ Ú©ÛŒ Ù‚Ø±Ø§Ø± Ú¯Ø°Ø§Ø´ØªÛŒÙ…ØŸ"
        print(f"\nQuestion 1: {question1}")
        response_tellme1 = await answer_question_from_chat_history(
            STANDALONE_TEST_API_KEY, 
            STANDALONE_TEST_MODEL, 
            sample_history_for_tellme,
            question1
        )
        print(f"AI Response for Question 1:\n{response_tellme1}")
        print("-" * 20)

        question2 = "Ø¯Ø± Ù…ÙˆØ±Ø¯ Ú†Ù‡ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒÛŒ ØµØ­Ø¨Øª Ø´Ø¯ØŸ"
        print(f"\nQuestion 2: {question2}")
        response_tellme2 = await answer_question_from_chat_history(
            STANDALONE_TEST_API_KEY, 
            STANDALONE_TEST_MODEL, 
            sample_history_for_tellme,
            question2
        )
        print(f"AI Response for Question 2:\n{response_tellme2}")
        print("-" * 20)

        question3 = "ØºØ²Ù„ Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…Ø§Ø´ÛŒÙ†Ø´ Ú†ÛŒ Ú¯ÙØªØŸ" # Question not in history
        print(f"\nQuestion 3: {question3}")
        response_tellme3 = await answer_question_from_chat_history(
            STANDALONE_TEST_API_KEY, 
            STANDALONE_TEST_MODEL, 
            sample_history_for_tellme,
            question3
        )
        print(f"AI Response for Question 3:\n{response_tellme3}")


    asyncio.run(run_standalone_tests())
