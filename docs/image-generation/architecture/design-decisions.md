# Design Decisions

**Last Updated:** 2024-01-15  
**Audience:** Architects, Technical Leads  
**Purpose:** Rationale behind key architectural choices

## Table of Contents

- [Decision Record Format](#decision-record-format)
- [Core Design Decisions](#core-design-decisions)
- [Implementation Decisions](#implementation-decisions)
- [Technology Choices](#technology-choices)
- [Trade-offs Analysis](#trade-offs-analysis)

## Decision Record Format

Each decision is documented with:
- **Context:** What situation required a decision
- **Decision:** What was chosen
- **Rationale:** Why this choice was made
- **Consequences:** Benefits and drawbacks
- **Alternatives:** What else was considered
- **Status:** Current status (Accepted, Superseded, etc.)

## Core Design Decisions

### DD-001: Separate Queues Per Model

**Status:** Accepted  
**Date:** 2024-01-10

#### Context

Users can generate images with two different models: Flux (fast) and SDXL (high-quality). Need to decide how to queue requests.

**Options Considered:**
1. Single global FIFO queue
2. Separate queue per model
3. Priority queue based on model
4. No queue (direct processing)

#### Decision

**Implement separate FIFO queues per model (Flux, SDXL).**

#### Rationale

**Problem with Single Queue:**
```
Global Queue: [Flux-A, SDXL-B, Flux-C]
Processing Flux-A (10s) â†’ OK
Processing SDXL-B (15s) â†’ Flux-C waits unnecessarily!
```

**With Separate Queues:**
```
Flux Queue: [A, C] â†’ Process independently
SDXL Queue: [B]    â†’ Process concurrently
Result: A and B process simultaneously
```

**Benefits:**
1. No cross-model blocking
2. Fair wait times per model
3. Concurrent processing opportunity
4. Users choose their trade-off (speed vs quality)

#### Consequences

**Positive:**
- âœ… Flux users never wait for SDXL
- âœ… SDXL users never wait for Flux
- âœ… Better resource utilization (concurrent processing)
- âœ… Clear separation of concerns
- âœ… Easy to add new models in future

**Negative:**
- âš ï¸ More complex queue management (2 queues vs 1)
- âš ï¸ Need to track 2 processing flags
- âš ï¸ Slightly more code to maintain

**Mitigations:**
- Code is well-structured and documented
- Complexity is manageable (~100 lines)
- Benefits far outweigh complexity cost

#### Alternatives Considered

**Alternative 1: Single Global Queue**
- âŒ Rejected: Cross-model blocking
- âŒ Unfair wait times
- âœ… Simpler implementation

**Alternative 2: Priority Queue**
- âŒ Rejected: Complex priority rules
- âŒ Potential starvation
- âŒ Not fair

**Alternative 3: No Queue (Direct Processing)**
- âŒ Rejected: Uncontrolled concurrency
- âŒ Resource exhaustion risk
- âŒ No FIFO guarantees

#### Validation

**Tested Scenarios:**
```python
# Test 1: Concurrent different models
/image=flux=cat  (starts immediately)
/image=sdxl=dog  (starts immediately, concurrent!)
Result: Both process at same time âœ“

# Test 2: Sequential same model
/image=flux=cat  (starts immediately)
/image=flux=dog  (waits for cat)
Result: FIFO maintained âœ“

# Test 3: Mixed requests
/image=flux=a
/image=sdxl=b
/image=flux=c
Result: a+b concurrent, c waits for a only âœ“
```

### DD-002: Mandatory Prompt Enhancement with Fallback

**Status:** Accepted  
**Date:** 2024-01-10

#### Context

User prompts vary widely in quality. Simple prompts like "cat" produce poor results without additional detail.

**Options Considered:**
1. Mandatory enhancement (always try)
2. Optional enhancement (user chooses)
3. No enhancement (use original)
4. Conditional enhancement (only short prompts)

#### Decision

**Always attempt prompt enhancement using LLM, fallback to original on failure.**

#### Rationale

**Problem:**
```
User input: "cat"
Direct to model: Generic, low-quality result

With enhancement:
"A beautiful orange tabby cat sitting on a windowsill,
 soft natural lighting, photorealistic style, detailed
 fur texture, peaceful atmosphere"
Result: Much better image quality
```

**Why Mandatory:**
- Most users don't know how to write effective prompts
- AI can add professional details consistently
- Improves results for all users
- Users can learn by seeing enhanced prompts

**Why Fallback:**
- LLM might be unavailable
- API might timeout
- Cost/quota considerations
- Must never block image generation

#### Consequences

**Positive:**
- âœ… Dramatically better image quality
- âœ… User-friendly (works for simple prompts)
- âœ… Educational (users see good prompts)
- âœ… Consistent enhancement logic
- âœ… Reliable (always returns valid prompt)

**Negative:**
- âš ï¸ Adds 3-5 seconds to each request
- âš ï¸ Requires LLM API (cost)
- âš ï¸ Might modify user intent (rare)
- âš ï¸ Dependency on external service

**Mitigations:**
- Fallback ensures reliability
- 3-5s acceptable for 15-25s total time
- Enhancement preserves core concept
- Graceful degradation when LLM unavailable

#### Implementation

```python
async def enhance_prompt(self, user_prompt: str) -> str:
    # Check if LLM configured
    if not self._ai_processor.is_configured:
        return user_prompt  # Fallback 1
    
    try:
        # Attempt enhancement
        enhanced = await self._ai_processor.execute_custom_prompt(...)
        
        # Validate response
        if not enhanced or not enhanced.strip():
            return user_prompt  # Fallback 2
        
        # Return enhanced
        return enhanced
        
    except Exception as e:
        self._logger.warning(f"Enhancement failed: {e}")
        return user_prompt  # Fallback 3 (always safe)
```

**Fallback Points:**
1. LLM not configured â†’ Original
2. Empty response â†’ Original
3. Any exception â†’ Original

**Result:** 100% reliability, 0% failures due to enhancement

#### Alternatives Considered

**Alternative 1: Optional Enhancement**
- âŒ Rejected: Users don't know when to use
- âŒ Complexity in UI/commands
- âœ… More control for users

**Alternative 2: No Enhancement**
- âŒ Rejected: Poor quality for simple prompts
- âœ… Faster (no LLM call)
- âœ… Simpler implementation

**Alternative 3: Conditional Enhancement**
- âŒ Rejected: Complex rules
- âŒ Inconsistent behavior
- âœ… Could save API calls

### DD-003: English UI Messages, Any Language Prompts

**Status:** Accepted  
**Date:** 2024-01-11

#### Context

SakaiBot primarily serves Persian users, but image generation is international feature.

**Options Considered:**
1. All Persian (messages and prompts)
2. All English (messages and prompts)
3. Persian messages, any language prompts
4. English messages, any language prompts

#### Decision

**English for all UI messages, support any language for prompts (including Persian).**

#### Rationale

**UI Messages in English:**
- Image generation is international technology
- Error messages, status updates â†’ English standard
- Easier for non-Persian users
- Consistent with industry standards
- AI models work well with English instructions

**Prompts in Any Language:**
- Users can use their native language
- AI models support multilingual prompts
- Persian users can describe in Persian
- More accessible globally

#### Examples

```bash
# Persian prompt with English UI
/image=flux=Ú¯Ø±Ø¨Ù‡ Ø²ÛŒØ¨Ø§
Status: "ğŸ¨ Processing image request with FLUX..."
Enhancement: Works (AI understands Persian)
Result: Image generated successfully

# English prompt
/image=flux=beautiful cat
Status: "ğŸ¨ Processing image request with FLUX..."
Result: Works perfectly

# Error in English
Rate limit: "âš ï¸ Rate limit exceeded - please wait and try again."
Not: "âš ï¸ Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø¯Ø±Ø®ÙˆØ§Ø³Øª - Ù„Ø·ÙØ§ ØµØ¨Ø± Ú©Ù†ÛŒØ¯"
```

#### Consequences

**Positive:**
- âœ… International audience
- âœ… Industry standard
- âœ… Technical terms clear
- âœ… Any language prompts
- âœ… Better AI model support

**Negative:**
- âš ï¸ Persian users need basic English
- âš ï¸ Inconsistent with other features (may be Persian)

**Mitigations:**
- Emojis make meaning clear (ğŸ¨, âš ï¸, âœ…)
- Simple English used
- Status updates are intuitive

### DD-004: Polling Loop Instead of Event-Based

**Status:** Accepted (with Future Improvement Plan)  
**Date:** 2024-01-10

#### Context

Handlers need to wait for their turn in queue. How should they wait?

**Options Considered:**
1. Polling loop (check every 2 seconds)
2. Event-based (asyncio.Event)
3. Callback system
4. Observer pattern

#### Decision

**Use polling loop with 2-second intervals (with plan to migrate to events).**

#### Rationale

**Why Polling (Current):**
- Simple to implement and understand
- Works reliably
- Easy to debug
- Provides regular status updates
- No complex event coordination

**Implementation:**
```python
while True:
    if queue.try_start_processing(request_id, model):
        break  # Our turn!
    
    # Update status
    position = queue.get_queue_position(request_id, model)
    await client.edit_message(msg, f"Position {position}...")
    
    await asyncio.sleep(2)  # Poll every 2 seconds
```

**Why 2 Seconds:**
- Short enough: Users see updates
- Long enough: Not excessive overhead
- Balance between responsiveness and efficiency

#### Consequences

**Positive:**
- âœ… Simple implementation
- âœ… Reliable and testable
- âœ… Regular user updates
- âœ… Easy to understand
- âœ… Works well in practice

**Negative:**
- âš ï¸ Slight inefficiency (unnecessary checks)
- âš ï¸ 2-second granularity delay
- âš ï¸ Multiple concurrent loops running

**Mitigations:**
- Overhead negligible (< 0.01% CPU)
- 2-second delay acceptable
- Loops are lightweight (async)

#### Future Improvement

**Planned Migration to Events:**
```python
# Future implementation
class ImageQueue:
    def __init__(self):
        self._flux_event = asyncio.Event()
        self._sdxl_event = asyncio.Event()
    
    def mark_completed(self, request_id, image_path):
        # ... existing code ...
        
        # Notify next waiter
        if request.model == "flux":
            self._flux_event.set()
        elif request.model == "sdxl":
            self._sdxl_event.set()

# Handler waits on event
async def wait_for_turn(request_id, model):
    event = queue._flux_event if model == "flux" else queue._sdxl_event
    
    while True:
        if queue.try_start_processing(request_id, model):
            break
        await event.wait()  # Sleep until notified
        event.clear()
```

**Benefits:**
- Immediate notification (no delay)
- No unnecessary polling
- More efficient

**Why Not Now:**
- Current solution works well
- Optimization not critical
- Can migrate incrementally

### DD-005: Temporary Local File Storage

**Status:** Accepted  
**Date:** 2024-01-10

#### Context

Generated images need temporary storage before sending to Telegram.

**Options Considered:**
1. Local disk (temp directory)
2. In-memory (bytes buffer)
3. Remote storage (S3, etc.)
4. No storage (stream directly)

#### Decision

**Save to local temp directory, delete after sending.**

#### Rationale

**Why Local Disk:**
- Telethon requires file path for send_file()
- Simple implementation
- Reliable and testable
- Easy debugging (can inspect files)
- Minimal dependencies

**Why Temporary:**
- Images not needed after sending
- Prevents disk bloat
- Privacy (no permanent storage)
- Automatic cleanup

#### Implementation

```python
# Save location
IMAGE_TEMP_DIR = "temp/images"

# Filename format
filename = f"image_{model}_{request_id}_{timestamp}.png"

# Full path
filepath = temp_dir / filename
# Example: temp/images/image_flux_a1b2c3d4_1705334567.png

# Cleanup after sending
Path(image_path).unlink(missing_ok=True)
```

#### Consequences

**Positive:**
- âœ… Simple and reliable
- âœ… Works with Telethon API
- âœ… Easy to debug
- âœ… No external dependencies
- âœ… Privacy-friendly (auto-delete)

**Negative:**
- âš ï¸ Disk I/O overhead
- âš ï¸ Requires disk space
- âš ï¸ Not shared across instances
- âš ï¸ Lost on crashes (before cleanup)

**Mitigations:**
- I/O fast for small files (2-5MB)
- Disk space modest (~100MB for 20 active)
- Single-instance deployment (no sharing needed)
- Rare crash scenario, temp files cleaned on restart

#### Alternatives Considered

**Alternative 1: In-Memory**
```python
image_bytes = response.content
await client.send_file(chat_id, image_bytes)
```
- âŒ Rejected: Telethon prefers file paths
- âœ… No disk I/O
- âš ï¸ Complex workarounds needed

**Alternative 2: Remote Storage (S3)**
- âŒ Rejected: Unnecessary complexity
- âŒ External dependency
- âŒ Cost
- âœ… Shared across instances

**Alternative 3: Direct Streaming**
- âŒ Rejected: Not supported by workers
- âœ… No temporary storage
- âš ï¸ Complex implementation

### DD-006: httpx Over requests

**Status:** Accepted  
**Date:** 2024-01-10

#### Context

Need HTTP client for worker API calls. SakaiBot uses asyncio extensively.

**Options Considered:**
1. httpx (async native)
2. requests (sync, popular)
3. aiohttp (async alternative)

#### Decision

**Use httpx.AsyncClient for all HTTP requests.**

#### Rationale

**Why httpx:**
- Native async/await support
- Modern Python 3.10+ design
- HTTP/2 support
- Connection pooling
- Excellent timeout control
- Good retry integration
- Similar API to requests

**Key Features Used:**
```python
httpx.AsyncClient(
    timeout=httpx.Timeout(
        connect=30,
        read=120,
        write=120,
        pool=30
    ),
    limits=httpx.Limits(
        max_keepalive_connections=5,
        max_connections=10
    ),
    follow_redirects=True
)
```

#### Consequences

**Positive:**
- âœ… True async (non-blocking)
- âœ… Connection reuse (efficient)
- âœ… Fine-grained timeout control
- âœ… Modern and maintained
- âœ… Integrates well with asyncio

**Negative:**
- âš ï¸ Newer library (less mature than requests)
- âš ï¸ Different API than requests (learning curve)

**Mitigations:**
- httpx is production-ready and stable
- Similar enough to requests
- Good documentation

#### Alternatives Considered

**Alternative 1: requests**
```python
response = requests.get(url, timeout=120)
```
- âŒ Rejected: Synchronous (blocks event loop)
- âœ… Very mature and popular
- âš ï¸ Would need threading workarounds

**Alternative 2: aiohttp**
- âœ… Async native
- âŒ Different session management
- âŒ Less intuitive API
- âš ï¸ More boilerplate code

### DD-007: Retry with Exponential Backoff

**Status:** Accepted  
**Date:** 2024-01-10

#### Context

Network requests to workers can fail transiently. Need retry strategy.

**Options Considered:**
1. No retry (fail immediately)
2. Fixed retry (same delay)
3. Exponential backoff
4. Adaptive retry

#### Decision

**Implement exponential backoff with 3 max retries.**

#### Configuration

```python
@retry_with_backoff(
    max_retries=3,
    base_delay=1.0,
    max_delay=10.0
)
async def generate_with_flux(prompt):
    # ... generation logic ...
```

**Retry Schedule:**
- Attempt 1: Immediate (0s)
- Attempt 2: After 1s
- Attempt 3: After 2s (1 * 2)
- Attempt 4: After 4s (2 * 2)

**Total max delay:** 7 seconds for retries

#### Rationale

**Why Exponential:**
- Gives transient errors time to resolve
- Reduces load on recovering service
- Standard industry practice
- Balances speed and reliability

**Why 3 Retries:**
- Most transient errors resolve quickly
- 4 total attempts reasonable
- Total time acceptable (< 10s overhead)
- Higher values = diminishing returns

**What Gets Retried:**
- Network errors (connection, timeout)
- 5xx server errors (temporary)
- Transient worker issues

**What Doesn't Get Retried:**
- 4xx client errors (bad request, auth)
- Content validation failures
- Successful responses

#### Consequences

**Positive:**
- âœ… Resilient to transient failures
- âœ… Better success rate
- âœ… Industry standard approach
- âœ… Configurable parameters

**Negative:**
- âš ï¸ Adds latency on failures
- âš ï¸ More API calls (cost)
- âš ï¸ Complex error handling

**Mitigations:**
- Only retries transient errors
- Max delay capped at 10s
- User sees single final error

#### Example Scenario

```
Attempt 1: ConnectionError (worker overloaded)
Wait 1s...

Attempt 2: TimeoutError (still recovering)
Wait 2s...

Attempt 3: 200 OK (Success!)

User experience: Slightly longer wait, but success
Without retry: Would have failed immediately
```

## Implementation Decisions

### DD-008: Singleton Queue Instance

**Status:** Accepted  
**Date:** 2024-01-10

#### Context

Queue state needs to be shared across all handlers.

**Decision:** Global singleton instance

```python
# Global instance
image_queue = ImageQueue()

# Usage everywhere
from src.ai.image_queue import image_queue
```

#### Rationale

- Single source of truth
- No coordination complexity
- Simple to use
- Matches single-instance deployment

### DD-009: Request ID Format

**Status:** Accepted  
**Date:** 2024-01-10

#### Decision

```python
request_id = f"img_{uuid.uuid4().hex[:8]}"
# Example: "img_a1b2c3d4"
```

#### Rationale

- Unique across all requests
- Short and readable
- Sortable (for logs)
- Prefix identifies type

### DD-010: Status Update Frequency

**Status:** Accepted  
**Date:** 2024-01-10

#### Decision

Update queue position every 2 seconds while waiting.

#### Rationale

- Responsive to users
- Not excessive (bandwidth)
- Telegram edit rate limit safe

## Technology Choices

### Python 3.10+

**Rationale:** Modern features (match/case, type hints), async improvements

### Pydantic for Configuration

**Rationale:** Validation, type safety, excellent DX

### Telethon for Telegram

**Rationale:** Userbot support, async native, mature

## Trade-offs Analysis

### Simplicity vs. Performance

**Choice:** Favor simplicity

**Examples:**
- Polling loop (simple) over events (optimal)
- In-memory queue (simple) over distributed (scalable)
- Local files (simple) over streaming (efficient)

**Rationale:** Current scale doesn't need optimization, simplicity aids maintenance

### Flexibility vs. Constraints

**Choice:** Constrained flexibility

**Examples:**
- Two models (constrained) vs. unlimited (flexible)
- FIFO ordering (constrained) vs. priority (flexible)
- Fixed timeouts (constrained) vs. adaptive (flexible)

**Rationale:** Constraints simplify implementation and user understanding

### User Control vs. Automation

**Choice:** Automation with transparency

**Examples:**
- Auto-enhance (automated) with caption showing result (transparent)
- Auto-queue (automated) with position updates (transparent)

**Rationale:** Best user experience with informed users

---

**Next:** Development guides for implementing and extending the system
